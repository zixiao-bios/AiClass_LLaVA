import requests
from PIL import Image
import torch
from transformers import AutoProcessor, LlavaForConditionalGeneration, BitsAndBytesConfig

# æœ¬åœ°æ¨¡å‹è·¯å¾„ï¼ˆéœ€æ›¿æ¢ä¸ºå®é™…è·¯å¾„ï¼‰
model_path = "/mnt/workspace/llava-1.5-7b-hf"
# model_path = "/mnt/workspace/lora_llava_finetuned"

# é…ç½®é‡åŒ–å‚æ•°
bnb_config = BitsAndBytesConfig(
    load_in_8bit=True,            # INT8 é‡åŒ–
    llm_int8_skip_modules=['vision_tower', 'multi_modal_projector'],  # è·³è¿‡è§†è§‰ç¼–ç å™¨é‡åŒ–
    llm_int8_threshold=6.0,        # æ¿€æ´»å€¼å¼‚å¸¸é˜ˆå€¼
)

# åŠ è½½æ¨¡å‹å‚æ•°è¯´æ˜ï¼š
# pretrained_model_name_or_path: æ¨¡å‹æœ¬åœ°è·¯å¾„
# torch_dtype=torch.float16: åŠç²¾åº¦æµ®ç‚¹èŠ‚çœ50%æ˜¾å­˜
# low_cpu_mem_usage=True: åˆ†ç‰‡åŠ è½½å‡å°‘å†…å­˜å³°å€¼
# quantization_config=bnb_config: æŒ‡å®šé‡åŒ–é…ç½®
model = LlavaForConditionalGeneration.from_pretrained(
    model_path,
    torch_dtype=torch.float16,
    low_cpu_mem_usage=True,
    quantization_config=bnb_config,
)

# ä½¿ç”¨é‡åŒ–æ—¶ï¼Œæ¨¡å‹ä¼šè‡ªåŠ¨ä¼ åˆ° GPUï¼Œä¸æ”¯æŒæ‰‹åŠ¨æŒ‡å®š
# model = model.to(0)
model.eval()

# åŠ è½½å¤„ç†å™¨å‚æ•°è¯´æ˜ï¼š
# ä¸æ¨¡å‹åŒè·¯å¾„ç¡®ä¿tokenizerå’Œimage_processorç‰ˆæœ¬åŒ¹é…
processor = AutoProcessor.from_pretrained(model_path)

# ä¸‹è½½å›¾ç‰‡å‚æ•°è¯´æ˜ï¼š
# stream=True: æµå¼ä¼ è¾“é¿å…å¤§æ–‡ä»¶å†…å­˜æº¢å‡º
img_url = "http://images.cocodataset.org/val2017/000000039769.jpg"
response = requests.get(img_url, stream=True)
raw_image = Image.open(response.raw)


while True:
    user_input = input("ğŸ¤—ï¼š")
    if user_input == "exit":
        break

    if user_input == "image":
        img_url = input("å›¾ç‰‡é“¾æ¥ï¼š")
        response = requests.get(img_url, stream=True)
        raw_image = Image.open(response.raw)
        continue

    conversation = [{
        "role": "user",
        "content": [
            {"type": "text", "text": user_input},
            {"type": "image"},
        ],
    }]
    prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)
    inputs = processor(
        images=raw_image,
        text=prompt,
        return_tensors='pt'
    ).to(0)
    output = model.generate(
        **inputs,
        max_new_tokens=200,
    )
    output_text = processor.decode(output[0], skip_special_tokens=True)
    # åªè¾“å‡º â€˜ASSISTANTâ€™ åé¢çš„æ–‡æœ¬
    print(f'ğŸ¤–ï¼š{output_text.split("ASSISTANT:")[-1].strip()}')
