import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
from transformers import AutoProcessor, LlavaForConditionalGeneration
from tqdm import tqdm
from pathlib import Path
import requests
from PIL import Image

from data.dataset import MyImageInstructionDataset
from utils import apply_lora, load_lora_weight
from lora_config import LoraConfig


# ä¿å­˜å¤šç§ loraï¼Œç”¨äºåŠ¨æ€åŠ è½½
lora_dict = {
    "base": {
        "path": None,
        "weight": None,
    },
    "anger": {
        "path": "/mnt/workspace/lora_llava_finetuned_manual/anger.bin",
        "weight": None,
    },
    "writer": {
        "path": "/mnt/workspace/lora_llava_finetuned_manual/writer.bin",
        "weight": None,
    }
}

# base model è·¯å¾„
MODEL_PATH = "/mnt/workspace/llava-interleave-qwen-0.5b-hf"

# lora é…ç½®ï¼Œå¿…é¡»ä¸ lora è®­ç»ƒæ—¶ä¸€è‡´
lora_config = LoraConfig()

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")


def main():
    # åŠ è½½æ¨¡å‹å’Œå¤„ç†å™¨
    model = LlavaForConditionalGeneration.from_pretrained(
        MODEL_PATH,
        torch_dtype=torch.bfloat16,
        low_cpu_mem_usage=True,
    )
    model.generation_config.pad_token_id = 151645
    processor = AutoProcessor.from_pretrained(MODEL_PATH)

    # åŠ å…¥ LoRA ç»“æ„
    model = apply_lora(model, lora_config.target_modules, lora_config.rank, lora_config.alpha)
    model = model.to(device)
    print('model:')
    print(model)

    # åŠ è½½ LoRA æƒé‡åˆ°å†…å­˜
    for each in lora_dict.values():
        if each["path"]:
            each["weight"] = torch.load(each["path"])

    model.eval()

    # ä¸‹è½½å›¾ç‰‡å‚æ•°è¯´æ˜ï¼š
    # stream=True: æµå¼ä¼ è¾“é¿å…å¤§æ–‡ä»¶å†…å­˜æº¢å‡º
    img_url = "http://images.cocodataset.org/val2017/000000039769.jpg"
    response = requests.get(img_url, stream=True)
    raw_image = Image.open(response.raw)


    while True:
        user_input = input("ğŸ¤—ï¼š")
        if user_input == "exit":
            break

        if user_input == "image":
            img_url = input("å›¾ç‰‡é“¾æ¥ï¼š")

            # å°è¯•è·å–å›¾ç‰‡
            try:
                response = requests.get(img_url, stream=True)
                raw_image = Image.open(response.raw)
                print("å›¾ç‰‡åŠ è½½æˆåŠŸ")
                continue
            except Exception as e:
                print(f"å›¾ç‰‡åŠ è½½å¤±è´¥ï¼š{e}")
                continue

        if user_input == "lora":
            # åŠ è½½æŒ‡å®šçš„ lora æƒé‡
            lora_name = input("lora åç§°ï¼š")
            if lora_name not in lora_dict:
                print(f"åä¸º {lora_name} çš„ lora æƒé‡ä¸å­˜åœ¨")
                continue

            load_lora_weight(model, lora_dict[lora_name]["weight"])
            continue

        # æ„é€ æ¨¡å‹è¾“å…¥
        conversation = [{
            "role": "user",
            "content": [
                {"type": "text", "text": user_input},
                {"type": "image"},
            ],
        }]
        prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)
        inputs = processor(
            images=raw_image,
            text=prompt,
            return_tensors='pt'
        ).to(0)

        # æ¨¡å‹æ¨ç†
        output = model.generate(
            **inputs,
            max_new_tokens=200,
        )

        # å°†è¾“å‡ºçš„ token è§£ç ä¸ºæ–‡æœ¬
        output_text = processor.decode(output[0], skip_special_tokens=True)

        # åªè¾“å‡º â€˜ASSISTANTâ€™ åé¢çš„æ–‡æœ¬
        print(f'ğŸ¤–ï¼š{output_text.split("assistant")[-1].strip()}')


if __name__ == "__main__":
    main()
